dataset:
  max_length: 512
  normalize: true
  num_folds: 5
  fold_index: 0
  dataloader_workers: -1
  random_seed: 42
  # 文章の欠損をなおす
  fix_sentences: true
  # 同じessay_id,discourse_text,discourse_typeのデータの削除
  drop_duplicates: true

model:
  # transformers.models.auto.modeling_auto.AutoModelForTokenClassification.from_pretrainedと
  # transformers.models.auto.tokenization_auto.AutoTokenizer.from_pretrainedに渡す引数
  transformer:
    pretrained_model_name_or_path: microsoft/deberta-v3-large
  decoding:
    beam_size: 4
    labels: [Ineffective, Adequate, Effective]
  num_reinit_layers: 0
  random_seed: 42

optim:
  # torch.optim.adamw.AdamWに渡す引数
  optimizer:
    lr: 3e-5
    betas: [0.9, 0.999]
    eps: 1e-6
    weight_decay: 0.01
  # transformers.optimization.get_schedulerに渡す引数
  scheduler:
    name: linear
    num_warmup_steps: 500
    # Trainerのmax_stepsにも渡してる
    num_training_steps: 5000

train:
  # WandBの名前と保存するモデル名に使用される
  name: deberta-v3-large
  model_path: ./models/
  batch_size: 8
  gradient_checkpointing: true
  save_best_checkpoint: true

  # pytorch_lightning.trainer.trainer.Trainerに渡す引数
  gpus: 1
  precision: 16
  max_grad_norm: 1.0
  validation_interval: 0.05
  evaluate_after_steps: 2500
  accumulate_grads: 1
  logging_interval: 10

